{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pulsar_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkshtPcdKrsY"
      },
      "source": [
        "def binary_classification_exec(epoch_count = 10, mb_size = 10, report=1, train_ratio = 0.6, val_ratio = 0.2):\n",
        "    binary_load_dataset()\n",
        "    init_param()\n",
        "    train_metrics_mean_row, val_metrics_row, test_metrics = train_and_test(epoch_count, \n",
        "                                                                                mb_size, \n",
        "                                                                                report, \n",
        "                                                                                train_ratio,\n",
        "                                                                                val_ratio)\n",
        "    return train_metrics_mean_row, val_metrics_row, test_metrics"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAobfkf_Ktbs"
      },
      "source": [
        "def binary_load_dataset():\n",
        "    # 신경망 설계할 때는 mini dataset 으로 진행합니다.\n",
        "    #with open('/content/pulsar_stars_mini.csv') as csvfile:\n",
        "    with open('/content/pulsar_stars.csv') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader)\n",
        "        rows = []\n",
        "        for row in csvreader:\n",
        "            rows.append(row)\n",
        "\n",
        "    global data, input_cnt, output_cnt\n",
        "\n",
        "    input_cnt, output_cnt = 8, 1\n",
        "    data = np.asarray(rows, dtype='float32')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xisQlTxhKwRb"
      },
      "source": [
        "def init_param():\n",
        "    global weight, bias\n",
        "\n",
        "    weight = np.random.normal(RND_MEAN, RND_STD, size = [input_cnt, output_cnt])\n",
        "    bias   = np.zeros([output_cnt])      \n",
        "\n",
        "    print(\"Initial weight Value : \\n{}\".format(weight))\n",
        "    print(\"Initial bias Value : \\n{}\".format(bias))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyNd4Nd0KyZD"
      },
      "source": [
        "def arrange_data(mb_size, train_ratio, val_ratio):\n",
        "\n",
        "    global shuffle_map, test_begin_index, val_begin_index\n",
        "\n",
        "    shuffle_map = np.arange(data.shape[0])\n",
        "\n",
        "    np.random.shuffle(shuffle_map)\n",
        "    mini_batch_step_count = int(data.shape[0] * train_ratio) // mb_size\n",
        "\n",
        "    val_begin_index  = mini_batch_step_count * mb_size\n",
        "    test_begin_index = int(val_begin_index + (val_ratio * data.shape[0]))\n",
        "    \n",
        "    return mini_batch_step_count    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7lR7z2rK1L9"
      },
      "source": [
        "def get_test_data():\n",
        "    test_data = data[shuffle_map[test_begin_index : ]]\n",
        "    return test_data[:,:-output_cnt], test_data[:,-output_cnt:]\n",
        "\n",
        "def get_val_data():\n",
        "    val_data = data[shuffle_map[val_begin_index : test_begin_index]]\n",
        "    return val_data[:,:-output_cnt], val_data[:,-output_cnt:]\n",
        "\n",
        "def get_train_data(mb_size, n):\n",
        "    from_idx = n * mb_size\n",
        "    to_idx   = (n + 1) * mb_size\n",
        "\n",
        "    train_data = data[shuffle_map[from_idx : to_idx]]\n",
        "    \n",
        "    return train_data[:,:-output_cnt], train_data[:,-output_cnt:]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx3t4nzcK3l4"
      },
      "source": [
        "def train_and_test(epoch_count, mb_size, report, train_ratio, val_ratio):\n",
        "\n",
        "    mini_batch_step_count = arrange_data(mb_size,train_ratio, val_ratio)\n",
        "\n",
        "    test_x, test_y = get_test_data()\n",
        "    val_x,  val_y  = get_val_data()\n",
        "\n",
        "    losses_mean_row = []\n",
        "    val_loss_row    = []\n",
        "\n",
        "    #losses_mean_row, accs_mean_row = [], []\n",
        "    #val_loss_row, val_acc_row      = [], []\n",
        "    \n",
        "    for epoch in range(epoch_count):\n",
        "\n",
        "        losses = []\n",
        "        #accs = []\n",
        "\n",
        "        for n in range(mini_batch_step_count):\n",
        "            train_x, train_y  = get_train_data(mb_size, n)\n",
        "           \n",
        "            loss, _           = run_train(train_x,train_y)\n",
        "               \n",
        "            losses.append(loss)\n",
        "            #accs.append(acc)\n",
        "\n",
        "        val_loss, val_acc = run_test(val_x, val_y)\n",
        "        val_loss_row.append(val_loss)\n",
        "        #val_acc_row.append(val_acc)  \n",
        "\n",
        "        if report > 0 and (epoch+1) % report == 0:\n",
        "            \n",
        "            print(\"Epoch {} : Train - Loss = {:.3f} / Val - Loss = {:.3f}, Acc = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1 = {:.3F}\".\\\n",
        "                  format(epoch+1, np.mean(losses), val_loss, val_acc[0], val_acc[1], val_acc[2], val_acc[3]))\n",
        "            \n",
        "        losses_mean = np.mean(losses) \n",
        "        #accs_mean = np.mean(accs)\n",
        "\n",
        "        losses_mean_row.append(losses_mean)  \n",
        "        #accs_mean_row.append(accs_mean)   \n",
        "\n",
        "\n",
        "\n",
        "    test_loss, test_acc = run_test(test_x, test_y)\n",
        "    \n",
        "    print(\"\\n\",\"=\" * 50, 'Final Test', '=' * 50)\n",
        "    print('\\nTest Acc = {:.3f}, Precision = {:.3f}, Recall = {:.3f}, F1 = {:.3F}'.\\\n",
        "          format(test_acc[0], test_acc[1], test_acc[2], test_acc[3]))\n",
        "    print('\\nLoss = {:.3f}'.format(test_loss))\n",
        "\n",
        "    #return [losses_mean_row, accs_mean_row], [val_loss_row, val_acc_row], [test_loss , test_acc]\n",
        "    return [losses_mean_row], [val_loss_row], [test_loss , test_acc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZKClMINLA8V"
      },
      "source": [
        "def forward_neuralnet(x):\n",
        "    y_hat = np.matmul(x, weight) + bias\n",
        "    return y_hat, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITy95OFFLC2Y"
      },
      "source": [
        "def relu(x):\n",
        "    return np.maximum(x,0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wP5cyheLFP7"
      },
      "source": [
        "def sigmoid_cross_entropy_with_logits(z,x):\n",
        "    return relu(x) - x * z + np.log(1+np.exp(-np.abs(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoboMkeHLHDs"
      },
      "source": [
        "def forward_postproc(output, y):\n",
        "    CEE  = sigmoid_cross_entropy_with_logits(y,output)\n",
        "    loss = np.mean(CEE)\n",
        "    \n",
        "    return loss, [y, output, CEE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o00ijv69LLjh"
      },
      "source": [
        "def backprop_neuralnet(G_output, x):\n",
        "    global weight, bias\n",
        "\n",
        "    x_transpose = x.transpose()\n",
        "    G_w = np.matmul(x_transpose, G_output)\n",
        "\n",
        "    G_b = np.sum(G_output, axis = 0)\n",
        "\n",
        "    weight -= LEARNING_RATE * G_w\n",
        "    bias   -= LEARNING_RATE * G_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL_MnWSNLOB1"
      },
      "source": [
        "def sigmoid_cross_entropy_with_logits_derv(z, x):\n",
        "    return -z + sigmoid(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyOnZY4ELPg5"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return np.exp(-relu(-x)) / (1.0 + np.exp(-np.abs(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cXQp0UNLRCI"
      },
      "source": [
        "def backprop_postproc():\n",
        "    \n",
        "    g_loss_entropy = 1.0/np.prod(CEE.shape)\n",
        "    g_entropy_output = sigmoid_cross_entropy_with_logits_derv(y,output)\n",
        "    \n",
        "    G_output = g_entropy_output * g_loss_entropy \n",
        "    \n",
        "    return G_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH-Hw_g-LTae"
      },
      "source": [
        "def backprop_postproc(aux_pp_y_output_CEE):\n",
        "    \n",
        "    y, output, CEE = aux_pp_y_output_CEE\n",
        "\n",
        "    g_loss_entropy = 1.0/np.prod(CEE.shape)\n",
        "    g_entropy_output = sigmoid_cross_entropy_with_logits_derv(y,output)\n",
        "    \n",
        "    G_output = g_entropy_output * g_loss_entropy \n",
        "    \n",
        "    return G_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vboxvpJLa1_"
      },
      "source": [
        "def eval_accuracy_numpy(output,y):\n",
        "\n",
        "    est_yes = np.greater(output,0)\n",
        "    ans_yes = np.greater(y, 0.5)\n",
        "\n",
        "    est_no = np.logical_not(est_yes) \n",
        "    ans_no = np.logical_not(ans_yes)\n",
        "  \n",
        "    tp = np.sum(np.logical_and(est_yes, ans_yes))\n",
        "    tn = np.sum(np.logical_and(est_no, ans_no))\n",
        "    fp = np.sum(np.logical_and(ans_no, est_yes))\n",
        "    fn = np.sum(np.logical_and(ans_yes, est_no))\n",
        "\n",
        "    accuracy = safe_div(tp+tn,tp+fp+fn+tn)\n",
        "    precision = safe_div(tp,tp+fp)\n",
        "    recall = safe_div(tp,tp+fn)\n",
        "    f1 = 2 * safe_div(recall*precision,recall+precision)\n",
        "    \n",
        "    return [accuracy, precision, recall, f1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuYmq2lXLePd"
      },
      "source": [
        "def run_train(x, y):\n",
        "    y_hat, aux_nn_x           = forward_neuralnet(x)\n",
        "    loss, aux_pp_y_output_CEE = forward_postproc(y_hat, y)\n",
        "\n",
        "    accuracy = eval_accuracy_numpy(y_hat, y)\n",
        "    \n",
        "    G_output = backprop_postproc(aux_pp_y_output_CEE)\n",
        "    backprop_neuralnet(G_output, aux_nn_x)\n",
        "    \n",
        "    return loss, accuracy\n",
        "\n",
        "def run_test(x, y):\n",
        "    y_hat, _ = forward_neuralnet(x)\n",
        "    loss,  _ = forward_postproc(y_hat, y)\n",
        "    accuracy = eval_accuracy_numpy(y_hat, y)\n",
        "    return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR2WFzfLLpLO"
      },
      "source": [
        "def safe_div(p, q):\n",
        "    p, q = float(p), float(q)\n",
        "    if np.abs(q) < 1.0e-20:\n",
        "        return np.sign(p)\n",
        "    return p / q"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}